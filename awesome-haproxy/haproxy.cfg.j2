global
    # Print logs to stdout so Cloudwatch gets it, max line length 4kB
    log stdout len 4096 format raw local0

    chroot /var/lib/haproxy
    pidfile /var/run/haproxy.pid
    user haproxy
    group haproxy
    stats socket /var/lib/haproxy/stats

defaults
    mode http
    log global
    option dontlognull
    # option http-server-close # close connections toward backend after each response https://cbonte.github.io/haproxy-dconv/2.3/configuration.html#4
    option forwardfor except 127.0.0.0/8
    {% if HAPROXY_HTTP_BUFFER_REQUEST -%}
    # slow POST attack mitigation https://www.haproxy.com/blog/what-is-a-slow-post-attack-and-how-turn-haproxy-into-your-first-line-of-defense/
    option http-buffer-request
    {% endif -%}
    no option logasap # delay logging until after response
    retries                 3
    timeout http-request    10s
    timeout queue           1m
    timeout connect         10s
    timeout client          1m
    timeout server          "${HAPROXY_TIMEOUT_SERVER}"
    timeout http-keep-alive "${HAPROXY_TIMEOUT_SERVER}" # This needs to be set to the same value as timeout server because ALB only has one setting for both
    timeout check           10s
    maxconn                 30000
    # This log format line is generated by following instrutions in log-format.json
    # DO NOT EDIT THIS LINE MANUALLY
    log-format '{"conn":{"act":%ac,"fe":%fc,"be":%bc,"srv":%sc},"queue":{"backend":%bq,"srv":%sq},"time":{"tq":%Tq,"tw":%Tw,"tc":%Tc,"tr":%Tr,"tt":%Tt,"ta":%Ta,"tu":%Tu},"termination_state":"%tsc","retries":%rc,"network":{"client_ip":"%ci","client_port":%cp,"frontend_ip":"%fi","frontend_port":%fp},"request":{"id":"%ID","method":"%HM","bytes":%U,"uri":"%[capture.req.uri,json(utf8s)]","protocol":"%HV","cookie":"%CC","header":{"host":"%[capture.req.hdr(0),json(utf8s)]","cf_connecting_ip":"%[capture.req.hdr(1),json(utf8s)]","x_forwarded_for":"%[capture.req.hdr(2),json(utf8s)]","x_amzn_trace_id":"%[capture.req.hdr(3),json(utf8s)]","referer":"%[capture.req.hdr(4),json(utf8s)]","useragent":"%[capture.req.hdr(5),json(utf8s)]","cf_ray":"%[capture.req.hdr(6),json(utf8s)]","x_datadog_trace_id":"%[capture.req.hdr(7),json(utf8s)]","x_datadog_parent_id":"%[capture.req.hdr(8),json(utf8s)]"},"location":{"timezone":"%[capture.req.hdr(9),json(utf8s)]","city":"%[capture.req.hdr(10),json(utf8s)]","country":"%[capture.req.hdr(11),json(utf8s)]","continent":"%[capture.req.hdr(12),json(utf8s)]","longitude":"%[capture.req.hdr(13),json(utf8s)]","latitude":"%[capture.req.hdr(14),json(utf8s)]","region":"%[capture.req.hdr(15),json(utf8s)]","region_code":"%[capture.req.hdr(16),json(utf8s)]","postal_code":"%[capture.req.hdr(17),json(utf8s)]"}},"name":{"backend":"%b","frontend":"%ft","server":"%s"},"response":{"status_code":%ST,"bytes":%B,"cookie":"%CS"}}'

listen prometheus
    bind :9000 # Listen on localhost:9000
    mode http
    no log # Disable logging for this listener
    http-request use-service prometheus-exporter if { path /metrics }

listen hapstats
    bind :9090 # Listen on localhost:9090
    mode http
    no log # Disable logging for this listener
    stats enable
    stats uri /stats

listen haproxy
    bind "*:${HAPROXY_LISTEN_PORT}"

    # This defines a health check URL *on the haproxy nodes*, not how it checks its upstreams! It's used by
    # the AWS ELB to check servers in the pool. Removing this makes the LBs stop sending traffic to the haproxies.
    monitor-uri /_haproxy_health_check

    # TEMPORARY STATS SITE FOR DEBUGGING
    # stats enable
    # stats uri /haproxy/stats
    # stats refresh 5s

    # For security reasons we drop the Server header from all responses across all backends.
    http-response del-header Server
    # We set a basic HSTS header
    http-response set-header Strict-Transport-Security "max-age=86400"
    # We rewrite any cookies to be HTTPS-only cookies
    http-response replace-header Set-Cookie (.*) \1;\ Secure if !{ res.hdr(Set-Cookie),lower -m sub secure }

    # Log these request headers in the access log
    capture request header Host 				len 100 # hdr(0)
    capture request header CF-Connecting-IP     len 40 # hdr(1), Cloudflare original client IP
    capture request header X-Forwarded-For 		len 120 # hdr(2), proxy chain
    capture request header X-Amzn-Trace-Id      len 150 # hdr(3), AWS X-Ray trace ID
    capture request header Referer              len 200 # hdr(4), referrer URL
    capture request header User-Agent           len 200 # hdr(5), user agent
    capture request header CF-Ray               len 22 # hdr(6), Cloudflare request ID
    capture request header X-Datadog-Trace-Id   len 128 # hdr(7), Datadog trace ID
    capture request header X-Datadog-Parent-Id  len 128 # hdr(8), Datadog parent span ID

    capture request header cf-timezone          len 20  # hdr(9)
    capture request header cf-ipcity            len 40  # hdr(10)
    capture request header cf-ipcountry         len 2  # hdr(11)
    capture request header cf-ipcontinent       len 2  # hdr(12)
    capture request header cf-iplongitude       len 20  # hdr(13)
    capture request header cf-iplatitude        len 20  # hdr(14)
    capture request header cf-region            len 40  # hdr(15)
    capture request header cf-region-code       len 10  # hdr(16)
    capture request header cf-postal-code       len 10  # hdr(17)

    # Log the request ID in the access log
    unique-id-format %{+X}o\ %ci_%cp%fi%fp%Ts%rt
    unique-id-header X-Req-ID
    http-response set-header X-Req-ID %ID

    # requests on dev domain are private and should not be indexed
    acl private_hostname hdr(Host) -i -m end .{{ AWESOME_DEV_DOMAIN | default:"example.dev" }}
    http-request set-var(txn.is_private_hostname) bool(true) if private_hostname
    http-response set-header x-robots-tag "noindex, nofollow" if { var(txn.is_private_hostname) -m bool }

    use_backend default

backend default
    server "${HAPROXY_APP_NAME}" "${HAPROXY_APP_HOST}:${HAPROXY_APP_PORT}"

# we need resolvers because upstreams on cloud sometimes change IPs and resolving them just at startup isn't enough
resolvers one-dot
    nameserver one-one 1.1.1.1:53
    nameserver one-zero 1.0.0.1:53
    hold valid 30s
